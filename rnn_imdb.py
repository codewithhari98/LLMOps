# -*- coding: utf-8 -*-
"""RNN_IMDB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QZRgm_AGGSGSBGWXUh1C7NTiQ1fKmqzj
"""

import numpy as np
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Step 1: Load and Preprocess Data
max_features = 5000  # Number of words to consider as features
maxlen = 200  # Cut texts after this number of words
(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)

# Reduce dataset size for faster training
num_samples = 1000
X_train, y_train = X_train[:num_samples], y_train[:num_samples]
X_test, y_test = X_test[:num_samples], y_test[:num_samples]

X_train = pad_sequences(X_train, maxlen=maxlen)
X_test = pad_sequences(X_test, maxlen=maxlen)

# Step 2: Initialize RNN Parameters
def initialize_parameters(input_size, hidden_size, output_size):
    Wxh = np.random.randn(hidden_size, input_size) * 0.01
    Whh = np.random.randn(hidden_size, hidden_size) * 0.01
    Why = np.random.randn(output_size, hidden_size) * 0.01
    bh = np.zeros((hidden_size, 1))
    by = np.zeros((output_size, 1))
    return Wxh, Whh, Why, bh, by

# Step 3: Define Forward Pass
def rnn_forward(X, Wxh, Whh, Why, bh, by):
    h = np.zeros((Whh.shape[0], 1))
    for t in range(X.shape[0]):
        x_t = np.zeros((Wxh.shape[1], 1))
        x_t[X[t]] = 1
        h = np.tanh(np.dot(Wxh, x_t) + np.dot(Whh, h) + bh)
    y = 1 / (1 + np.exp(-np.dot(Why, h) - by))  # Sigmoid activation
    return y, h

# Step 4: Define Loss Function
def loss_function(y_pred, y_true):
    return -np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))

# Step 5: Backpropagation Through Time (BPTT)
def rnn_backward(X, y_true, Wxh, Whh, Why, bh, by):
    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)
    dbh, dby = np.zeros_like(bh), np.zeros_like(by)

    y_pred, h = rnn_forward(X, Wxh, Whh, Why, bh, by)
    loss = loss_function(y_pred, y_true)

    dy = y_pred - y_true
    dWhy += np.dot(dy, h.T)
    dby += dy

    dh = np.dot(Why.T, dy)
    for t in reversed(range(X.shape[0])):
        x_t = np.zeros((Wxh.shape[1], 1))
        x_t[X[t]] = 1
        dht = (1 - h ** 2) * dh
        dWxh += np.dot(dht, x_t.T)
        dWhh += np.dot(dht, h.T)
        dbh += dht
        dh = np.dot(Whh.T, dht)

    return dWxh, dWhh, dWhy, dbh, dby, loss

# Step 6: Training the RNN
def train_rnn(X_train, y_train, epochs=5, learning_rate=0.01):
    input_size = max_features
    hidden_size = 64
    output_size = 1

    Wxh, Whh, Why, bh, by = initialize_parameters(input_size, hidden_size, output_size)

    for epoch in range(epochs):
        total_loss = 0
        for i in range(len(X_train)):
            X, y_true = X_train[i], y_train[i]
            dWxh, dWhh, dWhy, dbh, dby, loss = rnn_backward(X, y_true, Wxh, Whh, Why, bh, by)
            total_loss += loss

            Wxh -= learning_rate * dWxh
            Whh -= learning_rate * dWhh
            Why -= learning_rate * dWhy
            bh -= learning_rate * dbh
            by -= learning_rate * dby

        print(f'Epoch {epoch + 1}, Loss: {total_loss / len(X_train)}')

    return Wxh, Whh, Why, bh, by

# Step 7: Evaluate the RNN
def evaluate_rnn(X_test, y_test, Wxh, Whh, Why, bh, by):
    correct_predictions = 0
    for i in range(len(X_test)):
        X, y_true = X_test[i], y_test[i]
        y_pred, _ = rnn_forward(X, Wxh, Whh, Why, bh, by)
        if (y_pred > 0.5) == y_true:
            correct_predictions += 1
    accuracy = correct_predictions / len(X_test)
    return accuracy

# New function to display prediction for a specific input
def predict_and_display(index, X, y, Wxh, Whh, Why, bh, by):
    input_sequence = X[index]
    true_sentiment = "Positive" if y[index] == 1 else "Negative"

    y_pred, _ = rnn_forward(input_sequence, Wxh, Whh, Why, bh, by)
    predicted_sentiment = "Positive" if y_pred > 0.5 else "Negative"

    print(f"True sentiment: {true_sentiment}")
    print(f"Predicted sentiment: {predicted_sentiment} (confidence: {y_pred[0][0]:.4f})")

    # Decode the input sequence back to words
    word_index = imdb.get_word_index()
    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
    decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in input_sequence if i > 0])
    print(f"Decoded review: {decoded_review}")

# Step 8: Run Training and Evaluation
print("Training the RNN...")
Wxh, Whh, Why, bh, by = train_rnn(X_train, y_train)
accuracy = evaluate_rnn(X_test, y_test, Wxh, Whh, Why, bh, by)
print(f'Accuracy on test set: {accuracy * 100:.2f}%')

# Step 9: Display prediction for a sample input
print("\nPrediction for a sample input:")
sample_index = np.random.randint(0, len(X_test))
predict_and_display(sample_index, X_test, y_test, Wxh, Whh, Why, bh, by)